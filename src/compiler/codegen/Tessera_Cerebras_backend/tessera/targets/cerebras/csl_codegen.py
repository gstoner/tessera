from typing import Dict

CSL_HEADER = """\
// === CSL scaffold generated by Tessera ===
// Adjust types/attrs to the real CSL syntax for your SDK version.
"""

def emit_gemm_kernel(name: str, params: Dict[str, int]) -> str:
    M, N, K = params["M"], params["N"], params["K"]
    tile_m, tile_n, tile_k = params.get("tile_m", 64), params.get("tile_n", 64), params.get("tile_k", 32)
    return f"""{CSL_HEADER}
kernel {name}(
    in  f16 A[{M}][{K}],   // row-major
    in  f16 B[{K}][{N}],   // col-major preferred in practice
    out f16 C[{M}][{N}]
){{
    // Pseudocode: block GEMM (C = A @ B)
    // Allocate scratch in SRAM (sizes trimmed to tile dims).
    // NOTE: Replace with true CSL storage qualifiers.
    sram f16 As[{tile_m}][{tile_k}];
    sram f16 Bs[{tile_k}][{tile_n}];
    sram f32 Acc[{tile_m}][{tile_n}] = 0.0;

    for (int m0 = 0; m0 < {M}; m0 += {tile_m}) {{
      for (int n0 = 0; n0 < {N}; n0 += {tile_n}) {{
        // zero accumulators
        for (int i=0;i<{tile_m};++i)
          for (int j=0;j<{tile_n};++j) Acc[i][j] = 0.0f;

        for (int k0 = 0; k0 < {K}; k0 += {tile_k}) {{
          // memcpy tiles from host/global to SRAM
          memcpy(As, &A[m0][k0], sizeof(f16)*{tile_m}*{tile_k});
          memcpy(Bs, &B[k0][n0], sizeof(f16)*{tile_k}*{tile_n});
          barrier();

          // naive inner product
          for (int kk=0; kk<{tile_k}; ++kk) {{
            for (int i=0;i<{tile_m};++i) {{
              f16 a = As[i][kk];
              for (int j=0;j<{tile_n};++j) {{
                Acc[i][j] += (float)a * (float)Bs[kk][j];
              }}
            }}
          }}
          barrier();
        }}
        // store back
        for (int i=0;i<{tile_m};++i)
          for (int j=0;j<{tile_n};++j)
            C[m0+i][n0+j] = (f16)Acc[i][j];
      }}
    }}
}}
"""

def emit_flashattn_tiny(name: str, params: Dict[str, int]) -> str:
    B = params.get("B", 1)
    H = params.get("H", 2)
    S = params.get("S", 128)
    D = params.get("D", 64)
    BK = params.get("BK", 64)  # block length for sequence stream
    return f"""{CSL_HEADER}
// Extremely simplified FlashAttention-style block softmax matmul
kernel {name}(
  in  f16 Q[{B}][{H}][{S}][{D}],
  in  f16 K[{B}][{H}][{S}][{D}],
  in  f16 V[{B}][{H}][{S}][{D}],
  out f16 O[{B}][{H}][{S}][{D}],
  in  f32 scale
){{
  sram f16 q_blk[{D}];
  sram f16 k_blk[{BK}][{D}];
  sram f16 v_blk[{BK}][{D}];
  sram f32 score[{BK}];
  sram f32 prob[{BK}];

  for (int b=0; b<{B}; ++b) {{
    for (int h=0; h<{H}; ++h) {{
      for (int s=0; s<{S}; ++s) {{
        memcpy(q_blk, &Q[b][h][s][0], sizeof(f16)*{D});
        // init output accumulator
        for (int d=0; d<{D}; ++d) O[b][h][s][d] = (f16)0.0f;

        for (int kb=0; kb<{S}; kb += {BK}) {{
          memcpy(k_blk, &K[b][h][kb][0], sizeof(f16)*{BK}*{D});
          memcpy(v_blk, &V[b][h][kb][0], sizeof(f16)*{BK}*{D});
          barrier();

          // scores = q · K^T
          for (int i=0;i<{BK};++i) {{
            float acc = 0.0f;
            for (int d=0; d<{D}; ++d)
              acc += (float)q_blk[d] * (float)k_blk[i][d];
            score[i] = acc * scale;
          }}

          // softmax (naive, per block)
          float m = -1e30f;
          for (int i=0;i<{BK};++i) m = (score[i] > m) ? score[i] : m;
          float sum = 0.0f;
          for (int i=0;i<{BK};++i) {{
            float e = exp(score[i]-m);
            prob[i] = e; sum += e;
          }}
          for (int i=0;i<{BK};++i) prob[i] = prob[i]/sum;

          // O += prob · V
          for (int i=0;i<{BK};++i) {{
            float p = prob[i];
            for (int d=0; d<{D}; ++d) {{
              float out = (float)O[b][h][s][d];
              out += p * (float)v_blk[i][d];
              O[b][h][s][d] = (f16)out;
            }}
          }}
        }}
      }}
    }}
  }}
}}
"""

def emit_csl(kind: str, name: str, params: Dict[str, int]) -> str:
    if kind == "gemm":
        return emit_gemm_kernel(name, params)
    if kind == "flashattn_tiny":
        return emit_flashattn_tiny(name, params)
    raise ValueError(f"unknown kind: {kind}")
