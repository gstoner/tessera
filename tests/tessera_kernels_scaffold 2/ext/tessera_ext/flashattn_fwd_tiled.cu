#include "common.cuh"
#include <torch/extension.h>
#include <cmath>

template <typename T, int BM, int BN>
__global__ __launch_bounds__(256, 2)
void fa_fwd_tiled_kernel(const T* __restrict__ Q,
                         const T* __restrict__ K,
                         const T* __restrict__ V,
                         const float* __restrict__ mask,          // [B,H,S,S] or null
                         const float* __restrict__ dropout_mask,  // [B,H,S,S] or null
                         T* __restrict__ Out,
                         int B, int H, int S, int D,
                         float scale, float dropout_p, bool is_causal)
{
  // Block assigns a tile of BM queries for one (b,h); loops over K/V tiles of BN.
  const int b = blockIdx.z;
  const int h = blockIdx.y;
  const int i0 = blockIdx.x * BM;
  if (b>=B || h>=H || i0>=S) return;

  // Shared tiles for K and V
  extern __shared__ char smem_raw[];
  T* Ktile = reinterpret_cast<T*>(smem_raw);
  T* Vtile = reinterpret_cast<T*>(smem_raw + BN * D * sizeof(T));

  // Thread mapping: 256 threads â†’ BM rows; each thread strides D
  const int tid = threadIdx.x;
  const int lane = tid & 31;

  // Per-row (query) softmax state
  float m_row[BM];
  float l_row[BM];
  #pragma unroll
  for (int ii=0; ii<BM; ++ii){ m_row[ii] = -1e30f; l_row[ii] = 0.0f; }

  // Base pointers
  const int head_stride = S * D;
  const T* Qhead = Q + ((b*H + h) * head_stride);
  const T* Khead = K + ((b*H + h) * head_stride);
  const T* Vhead = V + ((b*H + h) * head_stride);
  T* Ohead = Out + ((b*H + h) * head_stride);

  // Output accumulators
  // Keep partial O for each row in registers (streamed by dim loop)
  // For simplicity, we stream-write directly after final pass instead.

  // Iterate over key tiles
  for (int j0 = 0; j0 < S; j0 += BN){
    if (is_causal && j0 > i0 + BM - 1) break;

    // Load K,V tile into shared
    {
      TESSERA_NVTX_RANGE_COLORED("LDS:KV", 0xFFAA7733);
      // load BN*D elements cooperatively
      for (int idx = tid; idx < BN * D; idx += blockDim.x){
        int j = idx / D;
        int d = idx % D;
        int jj = j0 + j;
        if (jj < S){
          Ktile[j*D + d] = Khead[jj*D + d];
          Vtile[j*D + d] = Vhead[jj*D + d];
        } else {
          Ktile[j*D + d] = (T)0;
          Vtile[j*D + d] = (T)0;
        }
      }
    }
    __syncthreads();

    // First pass on this tile: compute local max logits per row
    {
      TESSERA_NVTX_RANGE_COLORED("QK^T:max", 0xFF2288FF);
      for (int ii = 0; ii < BM; ++ii){
        int i = i0 + ii;
        if (i >= S) break;
        float m_local = -1e30f;
        for (int j = 0; j < BN; ++j){
          int jj = j0 + j;
          if (jj >= S) break;
          if (is_causal && jj > i) break;
          float dot = 0.0f;
          for (int d = tid; d < D; d += blockDim.x){
            dot += (float)Qhead[i*D + d] * (float)Ktile[j*D + d];
          }
          // warp reduce sum
          for (int offs=16; offs>0; offs>>=1)
            dot += __shfl_down_sync(0xffffffff, dot, offs);
          if (lane == 0){
            float z = dot * scale + (mask ? mask[(b*H + h)*S*S + i*S + jj] : 0.0f);
            m_local = fmaxf(m_local, z);
          }
        }
        if (lane == 0){
          float m_old = m_row[ii];
          m_row[ii] = fmaxf(m_old, m_local);
          // rescale l_row later in second pass
        }
      }
    }
    __syncthreads();

    // Second pass: accumulate exp-normalizers and partial Out
    {
      TESSERA_NVTX_RANGE_COLORED("softmax+PV", 0xFF66CC22);
      for (int ii = 0; ii < BM; ++ii){
        int i = i0 + ii;
        if (i >= S) break;
        float m_i = m_row[ii];
        float l_add = 0.0f;

        for (int j=0; j<BN; ++j){
          int jj = j0 + j;
          if (jj >= S) break;
          if (is_causal && jj > i) break;
          float dot = 0.0f;
          for (int d = tid; d < D; d += blockDim.x){
            dot += (float)Qhead[i*D + d] * (float)Ktile[j*D + d];
          }
          for (int offs=16; offs>0; offs>>=1)
            dot += __shfl_down_sync(0xffffffff, dot, offs);
          if (lane == 0){
            float z = dot * scale + (mask ? mask[(b*H + h)*S*S + i*S + jj] : 0.0f);
            float p = __expf(z - m_i);
            if (dropout_mask){
              float keep = dropout_mask[(b*H + h)*S*S + i*S + jj];
              float inv_keep = (dropout_p>0.f) ? (1.f/(1.f-dropout_p)) : 1.f;
              p *= keep * inv_keep;
            }
            // Accumulate into O[i,:] using shared V
            // Simple strided accumulation across dims by all threads
            for (int d = tid; d < D; d += blockDim.x){
              atomicAdd(reinterpret_cast<float*>(&Ohead[i*D + d]), p * (float)Vtile[j*D + d]);
            }
            l_add += p;
          }
        }
        if (lane == 0){
          // combine with previous tiles' l_row (online)
          float l_old = l_row[ii];
          float m_old = m_row[ii]; // previous m after last tile (already maxed)
          // when m stays same across tile loop, online formula simplifies; keep it simple:
          l_row[ii] = l_old + l_add;
        }
      }
    }
    __syncthreads();
  } // end tiles loop

  // Normalize O by l_row
  {
    TESSERA_NVTX_RANGE_COLORED("normalize O", 0xFFBB33DD);
    for (int ii=0; ii<BM; ++ii){
      int i = i0 + ii;
      if (i >= S) break;
      float rinv = 1.0f / (l_row[ii] + 1e-20f);
      for (int d = tid; d < D; d += blockDim.x){
        float v = (float)Ohead[i*D + d] * rinv;
        Ohead[i*D + d] = (T)v;
      }
    }
  }
}

void flashattn_fwd_tiled_launcher(at::Tensor Q, at::Tensor K, at::Tensor V,
                                  c10::optional<at::Tensor> attn_mask,
                                  c10::optional<at::Tensor> dropout_mask,
                                  double scale, double dropout_p, bool is_causal,
                                  at::Tensor Out)
{
  TORCH_CHECK(Q.is_cuda() && K.is_cuda() && V.is_cuda() && Out.is_cuda(), "CUDA tensors required");
  TORCH_CHECK(Q.scalar_type()==at::kFloat || Q.scalar_type()==at::kHalf, "float16/float32 only");
  TORCH_CHECK(Q.sizes()==K.sizes() && Q.sizes()==V.sizes() && Q.sizes()==Out.sizes(), "shape mismatch");
  int B = Q.size(0), H = Q.size(1), S = Q.size(2), D = Q.size(3);
  const float* mask_ptr = nullptr;
  const float* drop_ptr = nullptr;
  if (attn_mask.has_value() && attn_mask->defined()){
    TORCH_CHECK(attn_mask->scalar_type()==at::kFloat && attn_mask->sizes()==std::vector<int64_t>({B,H,S,S}), "mask [B,H,S,S] float");
    mask_ptr = attn_mask->data_ptr<float>();
  }
  if (dropout_mask.has_value() && dropout_mask->defined()){
    TORCH_CHECK(dropout_mask->scalar_type()==at::kFloat && dropout_mask->sizes()==std::vector<int64_t>({B,H,S,S}), "dropout_mask [B,H,S,S] float");
    drop_ptr = dropout_mask->data_ptr<float>();
  }

  dim3 grid(div_up(S, 64), H, B);
  dim3 block(256);
  size_t smem = (size_t)(64 * D * sizeof(float)) * 2; // K and V tiles (use float space for half as well)

  if (Q.scalar_type()==at::kFloat){
    smem = (size_t)(64 * D * sizeof(float)) * 2;
    fa_fwd_tiled_kernel<float,64,64><<<grid, block, smem>>>(
      Q.data_ptr<float>(), K.data_ptr<float>(), V.data_ptr<float>(),
      mask_ptr, drop_ptr, Out.data_ptr<float>(),
      B,H,S,D, (float)scale, (float)dropout_p, is_causal);
  } else {
    smem = (size_t)(64 * D * sizeof(at::Half)) * 2;
    fa_fwd_tiled_kernel<half,64,64><<<grid, block, smem>>>(
      reinterpret_cast<const half*>(Q.data_ptr<at::Half>()),
      reinterpret_cast<const half*>(K.data_ptr<at::Half>()),
      reinterpret_cast<const half*>(V.data_ptr<at::Half>()),
      mask_ptr, drop_ptr,
      reinterpret_cast<half*>(Out.data_ptr<at::Half>()),
      B,H,S,D, (float)scale, (float)dropout_p, is_causal);
  }
  CUDA_CHECK(cudaGetLastError());
}
